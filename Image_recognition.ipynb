{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86370e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 945 images, Test: 106 images\n",
      "  Batch shape: torch.Size([16, 3, 300, 300]) Labels: tensor([ 9,  2, 15,  9,  1])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) 재현성을 위한 시드 고정\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# 2) 기본 이미지 전처리\n",
    "#    • train: 약간의 augmentation + 정규화\n",
    "#    • test : 중앙 크롭 + 정규화\n",
    "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "# 3) 전체 데이터셋 로드 (transform=None)\n",
    "root_dir = '/home/najo/NAS/CV_PA2_Image_recognition/caltech20'\n",
    "full_dataset = datasets.ImageFolder(root=root_dir, transform=None)\n",
    "\n",
    "# 4) 파일 경로와 레이블 추출\n",
    "paths = [sample[0] for sample in full_dataset.samples]\n",
    "labels = [sample[1] for sample in full_dataset.samples]\n",
    "\n",
    "# 5) train/test 분할 (stratify 유지, test_size=0.2)\n",
    "train_idx, test_idx = train_test_split(\n",
    "    list(range(len(paths))),\n",
    "    test_size=0.1,\n",
    "    random_state=seed,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "# 6) Subset + transform 적용\n",
    "class SubsetWithTransform(Dataset):\n",
    "    def __init__(self, dataset, indices, transform):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.dataset.samples[self.indices[idx]]\n",
    "        image = self.dataset.loader(img_path)  # PIL.Image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "train_dataset = SubsetWithTransform(full_dataset, train_idx, train_transform)\n",
    "test_dataset  = SubsetWithTransform(full_dataset, test_idx,  test_transform)\n",
    "\n",
    "# 7) DataLoader 생성\n",
    "batch_size = 16\n",
    "num_workers = 4  # CPU 코어 수에 따라 조정\n",
    "\n",
    "def pad_collate(batch): # padding\n",
    "    imgs, labels = zip(*batch)\n",
    "    # 배치 내 최대 높이/너비\n",
    "    max_h = max(img.shape[1] for img in imgs)\n",
    "    max_w = max(img.shape[2] for img in imgs)\n",
    "    padded = []\n",
    "    for img in imgs:\n",
    "        c, h, w = img.shape\n",
    "        pad_h = max_h - h\n",
    "        pad_w = max_w - w\n",
    "        # (left, right, top, bottom) 순서로 패딩\n",
    "        img_padded = F.pad(img, (0, pad_w, 0, pad_h))\n",
    "        padded.append(img_padded)\n",
    "    return torch.stack(padded), torch.tensor(labels)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=pad_collate\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=pad_collate\n",
    ")\n",
    "\n",
    "# 8) 확인\n",
    "print(f\"Train: {len(train_dataset)} images, Test: {len(test_dataset)} images\")\n",
    "for imgs, lbls in train_loader:\n",
    "    print(\"  Batch shape:\", imgs.shape, \"Labels:\", lbls[:5])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8add027b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 추출된 기술자 수: (40432, 128)\n",
      "샘플링된 기술자 수: (20000, 128)\n"
     ]
    }
   ],
   "source": [
    "# 사전 정의: ImageNet 정규화 역변환용\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# 1. SIFT 생성\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "def extract_dense_sift(img_tensor, step_size=16, sift=sift):\n",
    "    # (1) 정규화 역변환 → [0,255] uint8\n",
    "    img = img_tensor.clone().cpu()\n",
    "    for c in range(3):\n",
    "        img[c] = img[c] * std[c] + mean[c]\n",
    "    img = (img * 255).byte().numpy()             # shape: (3,H,W)\n",
    "    img = np.transpose(img, (1,2,0))             # (H,W,3)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) # (H,W)\n",
    "\n",
    "    # (2) 격자 기반 키포인트 생성\n",
    "    H, W = gray.shape\n",
    "    kps = [\n",
    "        cv2.KeyPoint(x, y, step_size)\n",
    "        for y in range(0, H, step_size)\n",
    "        for x in range(0, W, step_size)\n",
    "    ]\n",
    "\n",
    "    # (3) 기술자 계산\n",
    "    _, des = sift.compute(gray, kps)\n",
    "    return des  # None 이 아닌 경우 (N_kp,128)\n",
    "\n",
    "# 2.1) 모든 배치에서 기술자 수집 (예: 첫 100개 이미지 혹은 전체)\n",
    "all_descriptors = []\n",
    "max_images = 100  # 메모리 상황에 맞춰 조정\n",
    "\n",
    "count = 0\n",
    "for imgs, _ in train_loader:  # train_loader 는 앞서 정의된 DataLoader\n",
    "    for img in imgs:\n",
    "        des = extract_dense_sift(img, step_size=16)\n",
    "        if des is not None:\n",
    "            all_descriptors.append(des)\n",
    "    count += len(imgs)\n",
    "    if count >= max_images:\n",
    "        break\n",
    "\n",
    "# 2.2) 하나의 (N_total,128) 배열로 병합\n",
    "all_descriptors = np.vstack(all_descriptors)\n",
    "print(\"총 추출된 기술자 수:\", all_descriptors.shape)  # e.g. (200000,128)\n",
    "\n",
    "# 2.3) 랜덤 샘플링\n",
    "num_samples = 20000\n",
    "if all_descriptors.shape[0] > num_samples:\n",
    "    idx = np.random.choice(all_descriptors.shape[0], num_samples, replace=False)\n",
    "    sampled_descriptors = all_descriptors[idx]\n",
    "else:\n",
    "    sampled_descriptors = all_descriptors\n",
    "\n",
    "print(\"샘플링된 기술자 수:\", sampled_descriptors.shape)  # (20000,128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e61c82fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 차원: (20000, 128)\n",
      "축소 후 차원: (20000, 64)\n",
      "Init 1/1 with method k-means++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inertia for init 1/1: 790033.625\n",
      "[MiniBatchKMeans] Reassigning 463 cluster centers.\n",
      "Minibatch step 1/200: mean batch inertia: 39.885340625\n",
      "Minibatch step 2/200: mean batch inertia: 37.404078125, ewa inertia: 37.404078125\n",
      "[MiniBatchKMeans] Reassigning 276 cluster centers.\n",
      "Minibatch step 3/200: mean batch inertia: 35.555059375, ewa inertia: 35.555151821315185\n",
      "[MiniBatchKMeans] Reassigning 309 cluster centers.\n",
      "Minibatch step 4/200: mean batch inertia: 35.02761875, ewa inertia: 35.0276451253348\n",
      "[MiniBatchKMeans] Reassigning 329 cluster centers.\n",
      "Minibatch step 5/200: mean batch inertia: 34.27101875, ewa inertia: 34.271056579427295\n",
      "[MiniBatchKMeans] Reassigning 327 cluster centers.\n",
      "Minibatch step 6/200: mean batch inertia: 34.625234375, ewa inertia: 34.62521666699562\n",
      "[MiniBatchKMeans] Reassigning 319 cluster centers.\n",
      "Minibatch step 7/200: mean batch inertia: 33.88554375, ewa inertia: 33.88558073179676\n",
      "[MiniBatchKMeans] Reassigning 321 cluster centers.\n",
      "Minibatch step 8/200: mean batch inertia: 33.6703375, ewa inertia: 33.67034826162351\n",
      "[MiniBatchKMeans] Reassigning 310 cluster centers.\n",
      "Minibatch step 9/200: mean batch inertia: 33.557959375, ewa inertia: 33.55796499416337\n",
      "Minibatch step 10/200: mean batch inertia: 33.82250625, ewa inertia: 33.82249302359853\n",
      "Minibatch step 11/200: mean batch inertia: 33.81751875, ewa inertia: 33.81751899870124\n",
      "[MiniBatchKMeans] Reassigning 179 cluster centers.\n",
      "Minibatch step 12/200: mean batch inertia: 33.345403125, ewa inertia: 33.34542672961346\n",
      "Minibatch step 13/200: mean batch inertia: 33.27214375, ewa inertia: 33.27214741396578\n",
      "[MiniBatchKMeans] Reassigning 188 cluster centers.\n",
      "Minibatch step 14/200: mean batch inertia: 33.716815625, ewa inertia: 33.71679339270107\n",
      "[MiniBatchKMeans] Reassigning 219 cluster centers.\n",
      "Minibatch step 15/200: mean batch inertia: 33.40329375, ewa inertia: 33.403309424198426\n",
      "[MiniBatchKMeans] Reassigning 244 cluster centers.\n",
      "Minibatch step 16/200: mean batch inertia: 33.14343125, ewa inertia: 33.14344424325905\n",
      "Minibatch step 17/200: mean batch inertia: 33.4604, ewa inertia: 33.46038415300451\n",
      "[MiniBatchKMeans] Reassigning 183 cluster centers.\n",
      "Minibatch step 18/200: mean batch inertia: 32.81305, ewa inertia: 32.81308236508939\n",
      "[MiniBatchKMeans] Reassigning 237 cluster centers.\n",
      "Minibatch step 19/200: mean batch inertia: 33.089290625, ewa inertia: 33.089276815277486\n",
      "[MiniBatchKMeans] Reassigning 237 cluster centers.\n",
      "Minibatch step 20/200: mean batch inertia: 33.34315625, ewa inertia: 33.34314355666293\n",
      "[MiniBatchKMeans] Reassigning 242 cluster centers.\n",
      "Minibatch step 21/200: mean batch inertia: 33.36094375, ewa inertia: 33.36094286003483\n",
      "[MiniBatchKMeans] Reassigning 251 cluster centers.\n",
      "Minibatch step 22/200: mean batch inertia: 33.04545, ewa inertia: 33.04546577385431\n",
      "[MiniBatchKMeans] Reassigning 255 cluster centers.\n",
      "Minibatch step 23/200: mean batch inertia: 32.8311125, ewa inertia: 32.83112321712784\n",
      "[MiniBatchKMeans] Reassigning 249 cluster centers.\n",
      "Minibatch step 24/200: mean batch inertia: 33.204715625, ewa inertia: 33.20469694631354\n",
      "[MiniBatchKMeans] Reassigning 239 cluster centers.\n",
      "Minibatch step 25/200: mean batch inertia: 33.137753125, ewa inertia: 33.13775647202372\n",
      "[MiniBatchKMeans] Reassigning 248 cluster centers.\n",
      "Minibatch step 26/200: mean batch inertia: 32.600353125, ewa inertia: 32.60037999382391\n",
      "[MiniBatchKMeans] Reassigning 247 cluster centers.\n",
      "Minibatch step 27/200: mean batch inertia: 33.20154375, ewa inertia: 33.20151369331503\n",
      "[MiniBatchKMeans] Reassigning 256 cluster centers.\n",
      "Minibatch step 28/200: mean batch inertia: 32.971390625, ewa inertia: 32.971402130578134\n",
      "[MiniBatchKMeans] Reassigning 254 cluster centers.\n",
      "Minibatch step 29/200: mean batch inertia: 33.107009375, ewa inertia: 33.10700259497678\n",
      "[MiniBatchKMeans] Reassigning 258 cluster centers.\n",
      "Minibatch step 30/200: mean batch inertia: 32.939115625, ewa inertia: 32.9391240189288\n",
      "Minibatch step 31/200: mean batch inertia: 33.031003125, ewa inertia: 33.03099853127438\n",
      "Minibatch step 32/200: mean batch inertia: 32.96018125, ewa inertia: 32.96018479068703\n",
      "[MiniBatchKMeans] Reassigning 182 cluster centers.\n",
      "Minibatch step 33/200: mean batch inertia: 32.9023625, ewa inertia: 32.90236539096998\n",
      "Minibatch step 34/200: mean batch inertia: 32.981003125, ewa inertia: 32.980999193309884\n",
      "Minibatch step 35/200: mean batch inertia: 32.55439375, ewa inertia: 32.55441507920571\n",
      "[MiniBatchKMeans] Reassigning 127 cluster centers.\n",
      "Minibatch step 36/200: mean batch inertia: 32.81034375, ewa inertia: 32.81033095420625\n",
      "[MiniBatchKMeans] Reassigning 187 cluster centers.\n",
      "Minibatch step 37/200: mean batch inertia: 32.834228125, ewa inertia: 32.8342269302012\n",
      "[MiniBatchKMeans] Reassigning 205 cluster centers.\n",
      "Minibatch step 38/200: mean batch inertia: 33.088609375, ewa inertia: 33.08859665651368\n",
      "[MiniBatchKMeans] Reassigning 223 cluster centers.\n",
      "Minibatch step 39/200: mean batch inertia: 32.85875625, ewa inertia: 32.85876774144575\n",
      "[MiniBatchKMeans] Reassigning 227 cluster centers.\n",
      "Minibatch step 40/200: mean batch inertia: 32.675, ewa inertia: 32.67500918792767\n",
      "[MiniBatchKMeans] Reassigning 237 cluster centers.\n",
      "Minibatch step 41/200: mean batch inertia: 32.80334375, ewa inertia: 32.80333733359272\n",
      "[MiniBatchKMeans] Reassigning 240 cluster centers.\n",
      "Minibatch step 42/200: mean batch inertia: 32.783709375, ewa inertia: 32.78371035634886\n",
      "[MiniBatchKMeans] Reassigning 247 cluster centers.\n",
      "Minibatch step 43/200: mean batch inertia: 32.697384375, ewa inertia: 32.69738869108326\n",
      "[MiniBatchKMeans] Reassigning 242 cluster centers.\n",
      "Minibatch step 44/200: mean batch inertia: 33.168759375, ewa inertia: 33.16873580764417\n",
      "[MiniBatchKMeans] Reassigning 240 cluster centers.\n",
      "Minibatch step 45/200: mean batch inertia: 32.65188125, ewa inertia: 32.65190709143582\n",
      "Converged (lack of improvement in inertia) at step 45/200\n"
     ]
    }
   ],
   "source": [
    "# 1) 차원 축소 후 남길 차원 수 선정\n",
    "#    보통 128 → 64 또는 128 → 32 정도로 줄이면 속도/표현력 균형이 좋습니다.\n",
    "n_components = 64\n",
    "\n",
    "# 2) PCA 모델 학습\n",
    "pca = PCA(n_components=n_components, whiten=True, random_state=seed)\n",
    "pca.fit(sampled_descriptors)  \n",
    "#    sampled_descriptors: (M,128) 형태의 numpy array\n",
    "\n",
    "# 3) 차원 축소 적용\n",
    "descriptors_reduced = pca.transform(sampled_descriptors)  \n",
    "#    결과: (M,64) 형태\n",
    "\n",
    "print(\"원래 차원:\", sampled_descriptors.shape)\n",
    "print(\"축소 후 차원:\",      descriptors_reduced.shape)\n",
    "\n",
    "# 4) 이후 단계: 이 reduced descriptors로 K-Means 수행\n",
    "\n",
    "K = 1000\n",
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=K,\n",
    "    batch_size=10000,\n",
    "    random_state=seed,\n",
    "    reassignment_ratio=0.01,\n",
    "    verbose=1\n",
    ")\n",
    "kmeans.fit(descriptors_reduced)\n",
    "visual_vocab = kmeans.cluster_centers_  # (K,64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ba6aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Test accuracy: 100.00%\n",
      "Confusion Matrix:\n",
      " [[4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         4\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         6\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       1.00      1.00      1.00         6\n",
      "           7       1.00      1.00      1.00         7\n",
      "           8       1.00      1.00      1.00         7\n",
      "           9       1.00      1.00      1.00         7\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      1.00      1.00         4\n",
      "          12       1.00      1.00      1.00         3\n",
      "          13       1.00      1.00      1.00         6\n",
      "          14       1.00      1.00      1.00         4\n",
      "          15       1.00      1.00      1.00         9\n",
      "          16       1.00      1.00      1.00         6\n",
      "          17       1.00      1.00      1.00         4\n",
      "          18       1.00      1.00      1.00         6\n",
      "          19       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           1.00       106\n",
      "   macro avg       1.00      1.00      1.00       106\n",
      "weighted avg       1.00      1.00      1.00       106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def encode_image_bow(img_tensor, sift, pca, kmeans, K, mean, std):\n",
    "    des = extract_dense_sift(img_tensor)           # (N_kp,128) or None\n",
    "    hist = np.zeros(K, dtype=int)\n",
    "    if des is not None and des.shape[0] > 0:\n",
    "        # (1) PCA 차원 축소\n",
    "        des_reduced = pca.transform(des)           # (N_kp, D_reduced)\n",
    "        # (2) K-means 할당\n",
    "        assignments = kmeans.predict(des_reduced)  # (N_kp,)\n",
    "        # (3) 히스토그램\n",
    "        hist, _ = np.histogram(assignments, bins=np.arange(K+1))\n",
    "    return hist\n",
    "\n",
    "# 1) 테스트 집합 전체 히스토그램 & 레이블 수집\n",
    "test_bow, test_labels = [], []\n",
    "for imgs, labels in test_loader:       # test_loader: 앞서 정의된 DataLoader\n",
    "    for img, label in zip(imgs, labels):\n",
    "        hist = encode_image_bow(img, sift, pca, kmeans, K, mean, std)\n",
    "        test_bow.append(hist)\n",
    "        test_labels.append(label)\n",
    "\n",
    "test_bow    = np.vstack(test_bow)      # (N_test, K)\n",
    "test_labels = np.array(test_labels)    # (N_test,)\n",
    "\n",
    "# 1) 정규화\n",
    "bow_features = normalize(test_bow, norm='l2')\n",
    "\n",
    "# 2) SVM 학습\n",
    "clf = LinearSVC(C=1.0, max_iter=10000, random_state=seed)\n",
    "clf.fit(bow_features, test_labels)\n",
    "print(\"Training accuracy:\", clf.score(bow_features, test_labels))\n",
    "\n",
    "# L2 정규화 (학습 때와 동일하게)\n",
    "test_bow_norm = normalize(test_bow, norm='l2')\n",
    "\n",
    "# 예측\n",
    "preds = clf.predict(test_bow_norm)     # clf: LinearSVC\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = np.mean(preds == test_labels)\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(test_labels, preds)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\",\n",
    "      classification_report(test_labels, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9220d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 정의된 것:, nn (NearestNeighbors on visual_dictionary), K, mean, std\n",
    "def tensor_to_gray(img_tensor, mean, std):\n",
    "    \"\"\"torch.Tensor [3,H,W] → grayscale numpy [H,W] uint8\"\"\"\n",
    "    img = img_tensor.clone().cpu()\n",
    "    for c in range(3):\n",
    "        img[c] = img[c] * std[c] + mean[c]\n",
    "    img = (img * 255).byte().numpy()\n",
    "    img = np.transpose(img, (1,2,0))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    return gray\n",
    "\n",
    "def extract_dense_sift_gray(gray, step_size=16, sift=cv2.SIFT_create()):\n",
    "    \"\"\"\n",
    "    numpy gray [H,W] → Dense SIFT descriptors (N_kp,128)\n",
    "    \"\"\"\n",
    "    H, W = gray.shape\n",
    "    kps = [cv2.KeyPoint(x, y, step_size)\n",
    "           for y in range(0, H, step_size)\n",
    "           for x in range(0, W, step_size)]\n",
    "    _, des = sift.compute(gray, kps)\n",
    "    return des  # None or (N_kp,128)\n",
    "\n",
    "def encode_spatial_pyramid(img_tensor, sift, nn, K,\n",
    "                           mean, std,\n",
    "                           levels=(0,1,2),\n",
    "                           step_size=16):\n",
    "    gray = tensor_to_gray(img_tensor, mean, std)\n",
    "    H, W = gray.shape\n",
    "\n",
    "    pyramid_hist = []\n",
    "    L = max(levels)\n",
    "\n",
    "    for l in levels:\n",
    "        num_cells = 2**l\n",
    "        cell_h = int(np.ceil(H / num_cells))\n",
    "        cell_w = int(np.ceil(W / num_cells))\n",
    "\n",
    "        # 각 셀마다 BoW 히스토그램\n",
    "        for i in range(num_cells):\n",
    "            for j in range(num_cells):\n",
    "                y0, y1 = i*cell_h, min((i+1)*cell_h, H)\n",
    "                x0, x1 = j*cell_w, min((j+1)*cell_w, W)\n",
    "                cell_gray = gray[y0:y1, x0:x1]\n",
    "                des = extract_dense_sift_gray(cell_gray, step_size, sift)\n",
    "\n",
    "                hist = np.zeros(K, dtype=float)\n",
    "                if des is not None:\n",
    "                    # 각 descriptor를 nearest word로 할당\n",
    "                    word_idx = nn.kneighbors(des, return_distance=False).reshape(-1)\n",
    "                    h, _ = np.histogram(word_idx, bins=np.arange(K+1))\n",
    "                    hist = h.astype(float)\n",
    "\n",
    "                # 레벨별 가중치: l=L 에 가장 큰 가중치, l=0 에 가장 작은 가중치\n",
    "                weight = 1.0 / (2**(L - l + 1)) if l < L else 1.0 / (2**0)\n",
    "                pyramid_hist.append(weight * hist)\n",
    "\n",
    "    # 모든 레벨, 모든 셀의 히스토그램을 이어붙임\n",
    "    return np.concatenate(pyramid_hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e8d555",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MiniBatchKMeans' object has no attribute 'kneighbors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:       \u001b[38;5;66;03m# test_loader: 앞서 정의된 DataLoader\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(imgs, labels):\n\u001b[0;32m----> 5\u001b[0m         vec \u001b[38;5;241m=\u001b[39m \u001b[43mencode_spatial_pyramid\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# train_loader나 test_loader에서 꺼낸 tensor\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43msift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msift\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkmeans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisual_vocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# 1×1, 2×2, 4×4 그리드\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m         vec_norm \u001b[38;5;241m=\u001b[39m normalize(vec\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m         pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(vec_norm)\n",
      "Cell \u001b[0;32mIn[6], line 49\u001b[0m, in \u001b[0;36mencode_spatial_pyramid\u001b[0;34m(img_tensor, sift, nn, K, mean, std, levels, step_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m hist \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(K, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m des \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# 각 descriptor를 nearest word로 할당\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     word_idx \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m(des, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     50\u001b[0m     h, _ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhistogram(word_idx, bins\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(K\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     51\u001b[0m     hist \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MiniBatchKMeans' object has no attribute 'kneighbors'"
     ]
    }
   ],
   "source": [
    "nn_index = NearestNeighbors(n_neighbors=1).fit(visual_vocab)\n",
    "\n",
    "# 1) 테스트 집합 전체 히스토그램 & 레이블 수집\n",
    "test_bow, test_labels = [], []\n",
    "for imgs, labels in test_loader:       # test_loader: 앞서 정의된 DataLoader\n",
    "    for img, label in zip(imgs, labels):\n",
    "        vec = encode_spatial_pyramid(\n",
    "                img_tensor=img,     # train_loader나 test_loader에서 꺼낸 tensor\n",
    "                sift=sift,\n",
    "                nn=kmeans,\n",
    "                K=visual_vocab.shape[0],\n",
    "                mean=mean, std=std,\n",
    "                levels=(0,1,2),          # 1×1, 2×2, 4×4 그리드\n",
    "                step_size=16\n",
    "            )\n",
    "        vec_norm = normalize(vec.reshape(1,-1), norm='l2')\n",
    "        pred = clf.predict(vec_norm)\n",
    "\n",
    "        test_bow.append(pred)\n",
    "        test_labels.append(label)\n",
    "\n",
    "test_bow    = np.vstack(test_bow)      # (N_test, K)\n",
    "test_labels = np.array(test_labels)    # (N_test,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ed7b54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/najo/.conda/envs/dip/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/najo/.conda/envs/dip/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted feature matrix shape: torch.Size([945, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1) 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2) 사전 학습된 VGG16 불러오기\n",
    "vgg = models.vgg16(pretrained=True).to(device)\n",
    "vgg.eval()  # inference 모드로 전환\n",
    "\n",
    "# 3) 마지막 합성곱 레이어까지만 사용하는 Feature Extractor 정의\n",
    "#    VGG16의 features 부분이 conv 연산만 모아둔 Sequential입니다.\n",
    "feature_extractor = vgg.features\n",
    "\n",
    "# 4) 가중치 고정(미세조정이 아니라 완전 추출만 할 경우)\n",
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 5) (예시) DataLoader 준비 — 이미 train_loader를 정의하셨다면 이 부분은 생략 가능합니다.\n",
    "#    입력 이미지는 VGG의 입력 크기(224×224)와 정규화에 맞춰야 합니다.\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std= [0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "# dataset = YourCustomDataset(root, transform=transform)\n",
    "# loader  = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# 6) 마지막 합성곱 계층 출력 추출\n",
    "all_features = []\n",
    "all_labels   = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        # conv_out: torch.Tensor of shape (B, C, H, W)\n",
    "        conv_out = feature_extractor(imgs)\n",
    "        # 필요에 따라 전역 풀링 또는 Flatten\n",
    "        # 예: 채널별 평균 풀링 (Global Average Pooling)\n",
    "        gap = nn.functional.adaptive_avg_pool2d(conv_out, (1,1))\n",
    "        feats = gap.view(gap.size(0), -1)  # (B, C)\n",
    "        all_features.append(feats.cpu())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "# 7) 하나의 텐서로 합치기\n",
    "all_features = torch.cat(all_features, dim=0)  # (N_images, 512)\n",
    "all_labels   = torch.cat(all_labels,   dim=0)\n",
    "\n",
    "print(\"Extracted feature matrix shape:\", all_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96807e1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/najo/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/najo/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/najo/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_308501/2518514220.py\", line 19, in __getitem__\n    hist = self.encode(img)                         # numpy (D_spm,)\n  File \"/tmp/ipykernel_308501/2518514220.py\", line 26, in <lambda>\n    lambda img: encode_spatial_pyramid(\nTypeError: encode_spatial_pyramid() got multiple values for argument 'levels'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 59\u001b[0m\n\u001b[1;32m     50\u001b[0m test_spm_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     51\u001b[0m     test_spm_ds,\n\u001b[1;32m     52\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# ── 3) 동작 확인\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m feats, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_spm_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch feature shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, feats\u001b[38;5;241m.\u001b[39mshape)   \u001b[38;5;66;03m# (batch_size, D_spm)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch labels shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, labels\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1465\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/dip/lib/python3.10/site-packages/torch/_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/najo/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/najo/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/najo/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_308501/2518514220.py\", line 19, in __getitem__\n    hist = self.encode(img)                         # numpy (D_spm,)\n  File \"/tmp/ipykernel_308501/2518514220.py\", line 26, in <lambda>\n    lambda img: encode_spatial_pyramid(\nTypeError: encode_spatial_pyramid() got multiple values for argument 'levels'\n"
     ]
    }
   ],
   "source": [
    "def normalize_vec(vec):\n",
    "    norm = np.linalg.norm(vec)\n",
    "    return vec / norm if norm > 0 else vec\n",
    "\n",
    "class BoWSpatialDataset(Dataset):\n",
    "    def __init__(self, base_dataset, encode_fn):\n",
    "        \"\"\"\n",
    "        base_dataset: transforms 적용된 이미지 + 레이블을 반환하는 Dataset\n",
    "        encode_fn: img_tensor → 1D numpy array (SPM BoW feature)\n",
    "        \"\"\"\n",
    "        self.base = base_dataset\n",
    "        self.encode = encode_fn\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.base[idx]                     # img: torch.Tensor [3,H,W]\n",
    "        hist = self.encode(img)                         # numpy (D_spm,)\n",
    "        hist = normalize_vec(hist)                      # L2 정규화\n",
    "        feat = torch.from_numpy(hist).float()           # torch.Tensor (D_spm,)\n",
    "        return feat, label\n",
    "\n",
    "train_spm_ds = BoWSpatialDataset(\n",
    "    train_dataset,\n",
    "    lambda img: encode_spatial_pyramid(\n",
    "        img, sift, pca, kmeans, K, mean, std, levels=(0,1,2), step_size=16\n",
    "    )\n",
    ")\n",
    "\n",
    "test_spm_ds = BoWSpatialDataset(\n",
    "    test_dataset,\n",
    "    lambda img: encode_spatial_pyramid(\n",
    "        img, sift, pca, kmeans, K, mean, std, levels=(0,1,2), step_size=16\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# ── 2) DataLoader 생성\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "\n",
    "train_spm_loader = DataLoader(\n",
    "    train_spm_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_spm_loader = DataLoader(\n",
    "    test_spm_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# ── 3) 동작 확인\n",
    "feats, labels = next(iter(train_spm_loader))\n",
    "print(\"Batch feature shape:\", feats.shape)   # (batch_size, D_spm)\n",
    "print(\"Batch labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "192f0d76",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/najo/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/najo/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/najo/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_308501/2205728748.py\", line 19, in __getitem__\n    hist = self.encode(img)                         # numpy (D_spm,)\n  File \"/tmp/ipykernel_308501/2205728748.py\", line 26, in <lambda>\n    lambda img: encode_spatial_pyramid(\nTypeError: encode_spatial_pyramid() got multiple values for argument 'levels'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m         lbls\u001b[38;5;241m.\u001b[39mappend(y\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mvstack(feats), np\u001b[38;5;241m.\u001b[39mconcatenate(lbls)\n\u001b[0;32m---> 27\u001b[0m train_spm_feats, train_spm_lbls \u001b[38;5;241m=\u001b[39m \u001b[43mloader_to_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_spm_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m test_spm_feats,  test_spm_lbls  \u001b[38;5;241m=\u001b[39m loader_to_array(test_spm_loader)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# ── 2) VGG-13/19 Feature Extractor 정의 ──\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 22\u001b[0m, in \u001b[0;36mloader_to_array\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloader_to_array\u001b[39m(loader):\n\u001b[1;32m     21\u001b[0m     feats, lbls \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     23\u001b[0m         feats\u001b[38;5;241m.\u001b[39mappend(X\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     24\u001b[0m         lbls\u001b[38;5;241m.\u001b[39mappend(y\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1465\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/dip/lib/python3.10/site-packages/torch/_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/najo/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/najo/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/najo/.conda/envs/dip/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_308501/2205728748.py\", line 19, in __getitem__\n    hist = self.encode(img)                         # numpy (D_spm,)\n  File \"/tmp/ipykernel_308501/2205728748.py\", line 26, in <lambda>\n    lambda img: encode_spatial_pyramid(\nTypeError: encode_spatial_pyramid() got multiple values for argument 'levels'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# ── 0) 공통 설정 ──\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# 이미 준비된 것들:\n",
    "# train_bow_norm, test_bow_norm, train_labels, test_labels      # BoW L2-normed arrays\n",
    "# train_spm_loader, test_spm_loader                              # BoW+SPM DataLoaders\n",
    "# train_loader, test_loader                                      # 원본 이미지 DataLoaders (VGG용)\n",
    "# device                                                         # torch.device\n",
    "\n",
    "# ── 1) BoW+SPM 특징 벡터를 NumPy 배열로 수집 ──\n",
    "def loader_to_array(loader):\n",
    "    feats, lbls = [], []\n",
    "    for X, y in loader:\n",
    "        feats.append(X.numpy())\n",
    "        lbls.append(y.numpy())\n",
    "    return np.vstack(feats), np.concatenate(lbls)\n",
    "\n",
    "train_spm_feats, train_spm_lbls = loader_to_array(train_spm_loader)\n",
    "test_spm_feats,  test_spm_lbls  = loader_to_array(test_spm_loader)\n",
    "\n",
    "# ── 2) VGG-13/19 Feature Extractor 정의 ──\n",
    "def make_vgg_extractor(arch):\n",
    "    vgg = getattr(models, arch)(pretrained=True).features.eval().to(device)\n",
    "    for p in vgg.parameters(): p.requires_grad=False\n",
    "    def extract(loader):\n",
    "        all_f, all_y = [], []\n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls in loader:\n",
    "                imgs = imgs.to(device)\n",
    "                conv = vgg(imgs)                                  # (B, C, H, W)\n",
    "                gap  = F.adaptive_avg_pool2d(conv, (1,1)).view(imgs.size(0), -1)\n",
    "                all_f.append(gap.cpu().numpy())\n",
    "                all_y.append(lbls.numpy())\n",
    "        return np.vstack(all_f), np.concatenate(all_y)\n",
    "    return extract\n",
    "\n",
    "vgg13_extract = make_vgg_extractor('vgg13')\n",
    "vgg19_extract = make_vgg_extractor('vgg19')\n",
    "\n",
    "train_vgg13_feats, train_vgg13_lbls = vgg13_extract(train_loader)\n",
    "test_vgg13_feats,  test_vgg13_lbls  = vgg13_extract(test_loader)\n",
    "\n",
    "train_vgg19_feats, train_vgg19_lbls = vgg19_extract(train_loader)\n",
    "test_vgg19_feats,  test_vgg19_lbls  = vgg19_extract(test_loader)\n",
    "\n",
    "# ── 3) SVM 학습 & 평가 함수 ──\n",
    "def train_and_eval(name, X_train, y_train, X_test, y_test):\n",
    "    svm = LinearSVC(C=1.0, max_iter=10000, random_state=seed)\n",
    "    svm.fit(X_train, y_train)\n",
    "    preds = svm.predict(X_test)\n",
    "    acc   = accuracy_score(y_test, preds)\n",
    "    print(f\"[{name}] Test accuracy: {acc*100:.2f}%\")\n",
    "    print(classification_report(y_test, preds, zero_division=0))\n",
    "    return svm\n",
    "\n",
    "# ── 4) 네 가지 특징으로 SVM 수행 ──\n",
    "svm_bow    = train_and_eval(\"BoW\",       train_bow_norm,    train_labels,\n",
    "                                          test_bow_norm,     test_labels)\n",
    "\n",
    "svm_bowspm = train_and_eval(\"BoW + SPM\", train_spm_feats,   train_spm_lbls,\n",
    "                                          test_spm_feats,    test_spm_lbls)\n",
    "\n",
    "svm_v13    = train_and_eval(\"VGG-13\",    train_vgg13_feats, train_vgg13_lbls,\n",
    "                                          test_vgg13_feats,  test_vgg13_lbls)\n",
    "\n",
    "svm_v19    = train_and_eval(\"VGG-19\",    train_vgg19_feats, train_vgg19_lbls,\n",
    "                                          test_vgg19_feats,  test_vgg19_lbls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d024fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
